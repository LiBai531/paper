---
title: LSTM压缩

---

## 简介

模型压缩对于拥有有限资源的用户设备和要求对大规模服务请求作出快速响应的业务集群的广泛采用递归神经网络（RNN）具有重要意义。这项工作旨在通过减少LSTM单元内的基本结构的大小，包括输入更新，门，隐藏状态，单元状态和输出来学习结构稀疏的长时间短期记忆（LSTM）。独立地减小基本结构的尺寸可能导致它们之间的尺寸不一致，并因此导致无效的LSTM单元。为了克服这个问题，我们在LSTM中提出了内部稀疏结构（ISS）。移除ISS的组件将同时将所有基础结构的尺寸减小一个，从而始终保持尺寸一致性。通过学习LSTM单元内的ISS，获得的LSTM保持规律，同时具有更小的基本结构。基于Group Lasso正则化，我们的方法实现了10.59倍的加速而不会失去Penn TreeBank数据集的语言建模困难。它也通过紧凑的模型成功评估，只有2.69M的重量用于SQUAD数据集的机器问题解答。我们的方法已成功扩展到非LSTM RNN，如循环高速公路网络（RHN）。

## 引言

模型压缩是一类缩减深度神经网络（DNNs）加速推理。结构学习作为DNN结构探索的一个活跃研究领域出现，潜在地用设备空间探索的机器自动化取代了人类劳动。在这两种技术的交集中，一个重要的领域是学习DNN中的紧凑结构，以利用最小的存储器和执行时间进行有效的推理计算，而不会降低准确性。卷积神经网络（CNN）中的紧凑结构学习在过去几年中得到了广泛的研究。 Han等人（2015b）提出了稀疏CNN的连接修剪。修剪法也可在粗粒度水平上成功完成，例如CNN中的修剪滤波器（Li et al（2017）），并减少神经元数量（Alvarez＆Salzmann（2016））。温等人（2016）提出了一个通用框架来学习DNN中的多功能紧凑结构（神经元，滤波器，滤波器形状，通道甚至层）。
学习循环神经网络（RNN）中的紧凑结构更具挑战性。由于循环单元在所有时间步骤中按顺序共享，因此压缩单位将严重影响所有步骤。 Narang等人最近的工作（2017）提出了一种修剪方法，删除RNN中高达90％的连接。连接修剪方法可以稀疏递归循环单元的权重，但不能明确更改基本结构，例如输入更新的数量，门，隐藏状态，单元状态和输出。此外，获得的稀疏矩阵具有非零权重的不规则/非结构化模式，这对于现代硬件系统中的有效计算是不友好的（Lebedev＆Lempitsky（2016））。先前关于GPU中稀疏矩阵乘法的研究（Wen et al（2016））表明，加速可能适得其反，或者可以忽略不计。更具体而言，在AlexNet的重量矩阵中，稀疏度分别为67.6％，92.4％，97.2％，96.6％和94.3％，加速度分别为0.25×，0.52×，1.38×，1.04×和1.36×。这个问题也存在于CPU中。图1显示稀疏性中的非结构化模式限制了加速。当稀疏度超过80％时，我们才开始观察速度增益，即使稀疏度为95％远远低于理论值20倍，加速度也是大约3倍到4倍。在这项工作中，我们专注于学习结构稀疏LSTMs来提高计算效率。更具体地说，我们的目标是在学习期间同时减少基本结构的数量，从而使得获得的LSTM具有原始示意图，其具有密集连接但是具有这些基本结构的较小尺寸。这种紧凑的模型具有结构化的稀疏性，去除了权重矩阵中的行和列，其计算效率如图1所示。此外，深度学习框架中的现成库可直接用于部署减小的LSTM。细节应该解释。

来自循环单元的重要挑战是：由于基本结构相互交织，独立地去除这些结构可能导致它们的尺寸不匹配，然后诱导无效的循环单元。 这个问题在CNN中不存在，神经元（或滤波器）可以在不违反最终网络结构的可用性的情况下被独立移除。 我们的主要贡献之一是确定RNN内部的结构，将其视为一组，以最有效地探索基本结构的稀疏性。 更具体地说，我们提出了内部稀疏结构（ISS）作为实现目标的组。 通过去除与ISS的一个部件相关的重量，（基本结构的）尺寸/尺寸同时减少一个。

我们用Penn Treebank数据集（Marcus et al（1993））和SQuAD数据集的机器问题回答（Rajpurkar et al（2016））的语言建模，通过LSTM和RHN评估了我们的方法。 我们的方法既可以在微调中进行，也可以从头开始进行培训。 在语言模型（Zaremba et al（2014））中，在具有两个隐藏尺寸为1500的LSTM层（即ISS的1500个组件）的RNN中，我们的方法获知第一个和第三个中的尺寸为373和315 第二个LSTMs对于同样的perplexity已经足够了。 推理时间加快10.59倍。 通过使用相同数量的时期从头开始进行培训可以获得结果。 直接训练373和315大小的LSTM不能达到同样的perplexity，这证明了学习ISS进行模型压缩的优势。 RHN模型（Zilly等（2017））和BiDAF模型（Seo等（2017））也获得了令人鼓舞的结果。

![1](https://i.loli.net/2018/04/17/5ad59e21a212d.png)

> 图1：使用非结构化和结构化稀疏矩阵乘法的加速。 Intel Xeon CPU E5-2673 v3 @ 2.40GHz的英特尔MKL实施中测量速度。 W·X的一般矩阵 - 矩阵乘法（GEMM）由cblas sgemm实现。 选择矩阵大小以反映LSTM中常用的GEMM。 例如，（a）表示LSTM中的GEMM，其隐藏大小为1500，输入大小为1500，批量大小为10.为了通过稀疏化加速GEMM，W被稀疏化。 在非结构化稀疏方法中，W随机稀疏化并编码为稀疏计算的压缩稀疏行格式（使用mkl scsrmm）; 在结构化稀疏方法中，W中的2k列和4k行被移除以匹配较小尺寸下更快的GEMM的相同稀疏度水平（即，去除的参数的百分比）。

## 相关工作

DNN压缩的一个主要方法是降低DNN内结构的复杂性。这些研究可分为三类：去除原始DNN中的冗余结构，近似DNN的原始功能（Denil等（2013），Jaderberg等（2014），Hinton等（2015），Lu等（2016），Prabhavalkar等（2016），Molchanov等（2017））和设计具有固有紧密结构的DNN（Szegedy等（2015），He等（2016），Wu等（2017），Bradbury等（2016））。我们的方法属于第一类。
前馈神经网络（FNNs）中冗余结构的去除研究，一般在CNN中得到了广泛的研究。基于$l_1$正则化（Liu et al。（2015），Park et al。（2017））或连接修剪（Han et al。（2015b），Guo et al。（2016）），连接数/参数大幅度降低。证明基于组套索的方法在减少CNN中的粗糙结构（例如神经元，滤波器，通道，滤波器形状，甚至层）方面是有效的（Wen等（2016），Alvarez＆Salzmann（2016），Lebedev ＆Lempitsky（2016），Yoon＆Hwang（2017））。例如，温等人（2016）在ResNet中将层数从32个减少到18个，而CIFAR-10数据集没有任何精度损失。 Narang等人最近的工作（2017年）推进RNN的连接修剪技术。它将Deep Speech 2（Amodei et al（2016））的大小从268 MB压缩到32 MB左右。然而，据我们所知，在RNNs中，除了细粒连接之外，还没有开展任何工作来减少粗粒结构。为弥补这一差距，我们的工作目标是开发一种方法，可以学习减少LSTM单元内的基本结构数量。在稀疏之后，最终的LSTM仍然是具有相同连接的规则的LSTM，但尺寸缩小了。
另一个相关的研究是FNN或CNN的结构学习。 Zoph＆Le（2017）使用强化学习来搜索良好的神经架构。 Philipp＆Carbonell（2017）通过使用Lasso正则化组合动态地添加和消除FNN中的神经元。科尔特斯等人（2017）逐渐将子网络添加到当前网络中，以逐步降低目标函数。所有这些工作都集中在寻找FNN或CNN中的最佳结构以获得分类准确性。相比之下，这项工作旨在学习LSTM中用于模型压缩的紧凑结构。

## 学习固有稀疏结构

### 1固有稀疏结构

LSTMs内的计算是（Hochreiter＆Schmidhuber（1997））

$i_t = σ (X_t · W_{xi} + h_{t−1} · W_{hi} + b_i)$

$f_t = σ (X_t · W_{xf} + h_{t−1} · W_{hf} + b_f)$

$o_t = σ (X_t · W{xo} + h_{t−1} · W_{ho} + b_o)​$

$u_t = tanh (X_t · W{xu} + h_{t−1} · W_{hu} + b_u)$

$c_t = f_t\odot c_{t−1} + i_t\odot  u_t$

$h_t = o_t\odot tanh(c_t)$

其中$\odot$是单元乘法，σ(·)是S函数，tanh(·)是双曲正切函数。向量是行向量。 $Ws$是加权矩阵，它将（隐藏状态$h_{t-1}$和输入$x_t$）级联转换为输入更新$u_t$和门($i_t，f_t和o_t$)。图2是Olah（2015）布局中的LSTM的示意图。 $Ws$的转换和相应的非线性函数用矩形块表示。我们的目标是在LSTM内缩小这种复杂结构的尺寸，同时保持原有的原理图。由于元素运算符（“⊕”和“⊗”），图2中沿蓝色带的所有矢量必须具有相同的维度。我们称这个约束为“维度一致性”。遵循维度一致性所需的向量包括输入更新，所有门，隐藏状态，单元状态和输出。请注意，隐藏状态通常是连接到分类器层或堆叠的LSTM层的输出。从图2中可以看出，矢量（沿着蓝色波段）相互交织，因此独立地从一个或几个矢量中移除单个分量可能会导致违反尺寸一致性。

![2](https://i.loli.net/2018/04/17/5ad59f6c4a7ea.png)

> 图2：LSTM单元中的内部稀疏结构（ISS）。

为了克服这个问题，我们提出了LSTM内的内部稀疏结构（ISS），如图2中的蓝色带所示。ISS的一个组成部分突出显示为白色条带。 通过减小ISS的尺寸（即蓝带的宽度），我们能够同时减小基本结构的尺寸。

![3](https://i.loli.net/2018/04/17/5ad59f93ecf51.png)

> 图3 应用ISS在权重矩阵中

要学习稀疏的ISS，我们转向权重稀疏。方程(1)中共有8个权重矩阵。我们以图3的形式将它们组织为TensorFlow中的基本LSTM单元。我们可以通过清除图3中白色行和白色列中的所有相关权重来删除ISS的一个组件。为什么？假设h的第k个隐藏状态是可移除的，则较低四个权重矩阵中的第k行可以全部为零（如图3中左侧白色水平线所示），因为那些权重在接收连接上第k个无用的隐藏状态。同样，下一层接收第k个隐藏状态的所有连接都可以被移除，如右边的白色水平线所示。请注意，下一层可以是输出层，LSTM层，全连接的层或它们的混合。 ISS覆盖两层或更多层，没有明确的解释，我们将第一个LSTM层称为ISS的所有权。当第k个隐藏状态变为无用时，产生该隐藏状态的第k个输出门和第k个单元状态是可移除的。由于第k个输出门由$W_{xo}$和$W_{ho}$中的第k列产生，这些权重可以被清零（如图3中的第四垂直白线所示）。追溯图2中的计算流程，我们可以得到类似于图3中第一，第二和第三垂直线分别显示的遗忘门，输入门和输入更新的结论。为了方便起见，我们称之为==权重以白色的行和列作为“ISS权重组”==。尽管我们在LSTM中提出了ISS，但也可以实现用于vanilla RNN，门控重复单元（GRU）（Cho等（2014））和循环高速路网（RHN）（Zilly等（2017））的ISS变体基于相同的理念。

即使是中等规模的LSTM，一个ISS权重组的权重数量也可能非常大。 同时kill如此多的权重以保持原始识别性能似乎非常凶。 然而，所提出的ISS本质上存在于LSTM中，并且甚至可以通过使用$l_1$-范数正则化来独立稀疏每个权重来揭示。 附录A介绍了实验结果。它揭示了稀疏ISS本质上存在于LSTM中，并且学习过程可以很容易地收敛到高ISS去除率的状态。 在3.2节中，我们提出了一种学习方法来显式移除比隐式的1范数正则化更多的ISS。

### 2方法

假设$w_k^{(n)}$是在第n个LSTM层中的ISS的第k个分量中的所有权重的向量(1≤n≤N且1≤k≤$K^{(n)}$)，其中N是LSTM层的数量，并且$K^{(n)}$是第n个LSTM层的ISS分量的数量（即，隐层大小）。优化目标是删除尽可能多的“ISS权重组”$w_k^{(n)}$并尽可能不失准确性。 如第2节所述，删除权重组的方法（如过滤器，通道和层）已成功地在CNN中进行了研究。但是，这些方法在RNN中的表现方式尚不清楚。 在这里，我们将基于Group Lasso的方法（Yuan＆Lin（2006））扩展到用于ISS稀疏学习的RNN。 更具体地说，为了提升ISS稀疏性，将Group Lasso正则化添加到最小化函数中。 形式上，ISS正则化是：

$R(w) = \sum_{n=1}^N\sum_{k=1}^{K^{(n)}}\left \| w_k^{(n)} \right\| _2$,               (2)

w是所有权重的向量，$\left \|\cdot \right \| $是$l_2$范式。在随机梯度下降（SGD）训练中，更新每个ISS权重组的步骤变为：

$w_k^{(n)} \leftarrow w_k^{(n)} - \eta \cdot (\frac{\partial E(w)}{\partial w_k^{(n)}} + \lambda \cdot \frac{w_k^{(n)}}{\left \|w_k^{(n)} \right \|_2})$,       (3)

其中E(w)是数据loss，η是学习率，λ> 0是Group Lasso正则化的系数，以权衡识别精度和ISS稀疏度。 正则化梯度，即方程（3）中的最后一项，是单位矢量。 它不断地将每个$w_k^{(n)}$的欧几里得长度压缩到零，因此，在学习之后，可以强制执行大部分ISS分量以完全零。 为了避免在正则化梯度计算中被零除，我们可以在$\left \|\cdot \right \|_2$中添加一个微小的数字$\epsilon$,也就是说，

$\left \|w_k^{(n)} \right \|_2  \doteq \sqrt{\epsilon + \sum_j(w_{kj}^{(n)})^2}$,                     (4)

其中$w_{kj}^{(n)}$是$w_k^{(n)}$的第j个元素，我们设置$\epsilon = 1.0e-8$，这种方法能有效地压缩了靠近零点的许多组，但由于始终存在波动的权重更新，因此很难将它们完全稳定为零。 幸运的是，波动在一个以零为中心的小球内。 为了稳定训练期间的稀疏性，我们将绝对值小于预定阈值τ的权重置零。 阈值处理的过程是每个mini-batch应用的。

## 实验

我们的实验使用已发布的模型作为baseline。应用领域包括Penn TreeBank的语言建模和SQuAD数据集的机器问题解答。为了进行更全面的评估，我们在LSTM模型中将ISS稀疏化，同时具有1500的大隐藏尺寸和100的小隐藏尺寸。我们还将ISS方法扩展到最先进的RHN（Zilly （2017））来减少每层单元的数量。我们最大化阈值τ以充分利用好处。对于特定的应用，我们通过交叉验证来预设τ。选择在不降低其性能的情况下稀疏稠密模型（baseline）的最大τ。 τ的验证只执行一次，不需要训练。对于Penn TreeBank中的堆叠LSTM，τ是1.0e-4，对于RHN和BiDAF模型，τ是4.0e-4。我们使用了Rasley等人的HyperDrive。 （2017）探索λ的超参数。更多细节可以在我们的源代码中找到。
为了测量推理速度，实验在双插槽Intel Xeon CPU E5-2673 v3 @ 2.40GHz处理器上运行，总共24个内核（每插槽12个）和128GB内存。英特尔MKL库2017更新2用于矩阵乘法操作。使用OpenMP运行时进行并行处理。我们使用英特尔C ++编译器17.0生成在Windows Server 2016上运行的可执行文件。每个实验运行1000次迭代，执行时间平均以查找执行延迟。

### 语言模型

#### 1 STACKED LSTMS

具有用于语言建模的两个堆叠LSTM层的RNN（Zaremba等（2014））被选作baseline。它在两个LSTM单元中具有1500个隐藏尺寸（即ISS的1500个组件）。输出层有一个10000字的词汇表。输入层中字嵌入的维数为1500.由于从矩阵中选择矢量的计算非常有效，因此字嵌入层未被稀疏化。采用与baseline相同的训练方案来学习ISS稀疏性，除了较大的dropout保持比率为baseline的0.6到0.35，因为Group Lasso正则化也可以避免过度拟合。所有模型都从零开始训练55个周期。结果如表1所示。需要注意的是，当使用dropout保持比率0.6进行训练而未采用Group Lasso正则化时，baseline过拟合和最低验证困惑度为97.73。困惑和稀疏的权衡由λ控制。在第二行中，与baseline具有微小的困惑差异，我们的方法可以将第一个和第二个LSTM单元中的ISS数量分别从1500减少到373和315。它将模型尺寸从66.0M减少到21.8M，并实现了10.59倍的加速。值得注意的是，如表1所示，实际加速比（10.59×）甚至超出了理论上的（7.48×），这是来自提高的计算效率。当应用结构化稀疏时，基础权重矩阵变小，以适应具有良好局部性的L3缓存，这改进了FLOPS（每秒浮点运算）。这是我们关于连接修剪产生的非结构稀疏RNN（Narang等人（2017））的一种关键优势，其遭受不规则内存访问模式和劣势理论加速。最后，当学习一个紧凑的结构时，我们的方法可以执行结构正则化来避免过度拟合。如表1第三行所示，即使是较小（25.2M）和更快（7.10X）的模型也可以实现较低的困惑度。其学习权重矩阵在图4中可见，其中白色条带显示的1119和965 ISS分量分别在第一和第二LSTM中被移除。

![](https://i.loli.net/2018/04/17/5ad59fbda2842.png)

![4](https://i.loli.net/2018/04/17/5ad59fd70f076.png)

> 图4：Group Lasso规则化学习的内在稀疏结构（放大以获得更好的视图）。 绘制原始的权重矩阵，其中蓝点是非零权重，白色是零。 为了更好的可视化，原始矩阵均匀地下采样10×10。

减少模型复杂性的直接方法是直接设计一个隐藏尺寸较小的RNN，并从零开始训练。 与直接设计方法相比，我们的ISS方法可以自动学习LSTM内的最佳结构。 更重要的是，与直接设计方法相比，ISS方法学习的紧凑模型具有更低的困惑度。 为了评估它，我们直接设计一个与表1中第二个RNN结构完全相同的RNN，并且从零开始进行训练，而不是从更大的RNN学习ISS。 结果列于表1的最后一行。我们调整了丢失保持率，以便为直接设计的RNN获得最佳的困惑度。 最后的测试困惑是85.66，比我们的ISS方法高出7.01。

#### 2扩展到RHN

RHN（Zilly等（2017））是一类最先进的循环模型，能够实现“一步到一步的深度超过一个”。在RHN中，我们将每层的单元数定义为RHN宽度。具体而言，我们选择Zilly等人的表1中的“变分RHN + WT”模型。 （2017年）作为基准。它的深度为10，宽度为830，参数总共为23.5M。简而言之，我们的方法可以将RHN宽度从830减少到517，而不会丢失困惑。
在确定“ISS权重组”以减少LSTM中基本结构的大小之后，我们可以确定RHN中的组以减少RHN宽度。简而言之，一组包括H非线性变换，T和C门以及嵌入层和输出层的权重矩阵中的对应列/行。组大小为46520.这些组在我们的源代码中由JSON文件指示4。通过在RHNs中学习ISS，我们可以同时减少字嵌入的维数和每层单位的数量。
表2总结了结果。除了在ISS学习中使用较小的dropout率之外，所有实验均从基线的相同超参数从头开始进行训练。 λ越大，RHN宽度越小但困惑度越高。更重要的是，我们的方法不失迷惑，可以从RHN宽度830的初始模型中学习一个RHN宽度为517的较小型号。这将模型大小减少到11.1M，减少了52.8％。此外，ISS学习可以找到一个宽度为726的较小的RHN模型，同时改善最新的困惑性，如表2的第二项所示。

![](https://i.loli.net/2018/04/17/5ad59ff40beea.png)

### 机器阅读理解

我们通过最先进的数据集（SQuAD）和模型（BiDAF）评估ISS方法。 SQuAD（Ra-jpurkar et al（2016））是最近发布的阅读理解数据集，从500多个维基百科文章中的100,000多个问题 - 答案对聚集在一起。 ExactMatch（EM）和F1分数是任务5的两个主要指标。 这些分数越高，模型越好。 我们采用BiDAF（Seo et al（2017））来评估ISS方法在小型LSTM单元中的工作方式。 BiDAF是一款紧凑的机器问答模型，共有2.69M重量。 所有LSTM装置的ISS尺寸只有100个。 BiDAF的实施由其作者6提供。

BiDAF具有字符，单词和上下文嵌入层，用于从输入句子中提取表示，然后是双向关注层，建模层和最终输出层。 LSTM单元用于上下文嵌入层，建模层和输出层。 所有LSTM都是双向的（Schuster＆Paliwal（1997））。 在双向LSTM中，有一个正向加一个反向LSTM分支。 这两个分支共享输入，并将它们的输出连接起来用于下一个堆叠层。 我们发现很难移除上下文嵌入层中的ISS组件，因为表示相对密集，因为它接近于输入并且原始隐藏大小（100）相对较小。 在我们的实验中，我们排除了上下文嵌入层中的LSTM，并且稀疏了所有其他LSTM层。 那些LSTM层是BiDAF的计算瓶颈。

我们在CPU上分析了计算时间，发现那些LSTM层（不包括上下文的embedding层）占总推理时间的76.47％。 有三个我们将稀疏化的双向LSTM层，其中两个属于建模层，另一个属于输出层。 BiDAF的更多细节由Seo等人涵盖。（2017年）。 为简洁起见，我们将建模层中第一个双向LSTM的前向（后向）路径标记为ModFwd1（ModBwd1）。 类似地，ModFwd2和ModBwd2用于第二个双向LSTM。 正向（反向）输出层中的LSTM路径标记为OutFwd和OutBwd。

正如3.1节所讨论的那样，多个并行层可以从同一个LSTM层接收隐藏状态，所有连接（权重）接收属于同一个ISS的隐藏状态。 例如，ModFwd2和ModBwd2都接收ModFwd1的隐藏状态作为输入，因此第k个“ISS权重组”包括ModFwd2和ModBwd2中的第k行权重，加上第k个ISS组件中的权重ModFwd1。 为了简单起见，我们使用“ModFwd1的ISS”来指代整组权重。 附录B的表5中包含了6个ISS的结构。我们通过微调baseline和从头开始的培训，了解了BiDAF中的ISS稀疏性。 所有训练计划与baseline保持一致，除了采用较高的dropout保持率。 训练结束后，我们将绝对值小于0.02的权重归零。 这不会影响EM和F1分数，但会增加稀疏度。

表3显示了EM，F1，剩余ISS组件的数量，模型大小和推理速度。 第一行是baseline BiDAF。 其他行通过使用ISS正则化对baseline进行微调来获得。 在第二行中，通过学习ISS，小EM和F1损失，我们可以减少除ModFwd1以外的所有LSTM中的ISS。 例如，在OutBwd中几乎有一半的ISS组件被删除。 通过增加Lasso正则化组（λ）的强度，我们可以通过丢失一些EM / F1得分来增加ISS的稀疏性。 权衡在表3中列出。在2.63 F1得分损失的情况下，OutFwd和OutBwd的大小可分别从原始100减小到15和12。 最后，我们发现很难在不损失任何EM / F1分数的情况下减少ISS规模。 这意味着BiDAF足够紧凑，其规模适合计算和准确性。 然而，我们的方法仍然可以在可接受的性能损失下显着压缩这个紧凑型模型。

![](https://i.loli.net/2018/04/17/5ad5a0127aa65.png)

最后，我们没有对baseline进行微调，而是通过ISS学习从头开始对BiDAF进行训练。 结果总结在表4中。我们的方法在从头开始训练时也可以很好地工作。 总的来说，从零开始的训练比微调更好地平衡了所有层的稀疏性，这样可以更好地压缩模型大小并加快推理时间。 “ISS权重组”的矢量长度直方图绘制在附录C中。

![](https://i.loli.net/2018/04/17/5ad5a0290ad53.png)

## 结论

我们提出了LSTM内部的内部稀疏结构（ISS）及其学习方法，以在复杂的LSTM结构内同时减少输入更新，门，隐藏状态，单元状态和输出的大小。 通过学习ISS，可以获得结构上稀疏的LSTM，其实质上是具有减小的隐藏维数的常规LSTM。 因此，不需要软件或硬件特定的定制来获得存储节省和计算加速。 虽然ISS是与LSTMs一起提出的，但它可以很容易地扩展到vanilla RNN，GRU（Cho等（2014））和RHNs（Zilly等（2017））。

## 附录

### APPENDIX A 通过$l_1$正则化揭示的内部稀疏结构

![A](https://i.loli.net/2018/04/17/5ad5a04cb11b3.png)

图5：通过$l_1$正则化揭示的内部稀疏结构（放大以获得更好的视图）。 第一行显示原始权重矩阵，其中蓝色点是非零权重，白色是零; 最后一行是图3格式的权重矩阵，其中白色条是ISS组件，其权重全为零。 为了更好的可视化，原始矩阵被均匀地下采样10×10。

我们采用Zaremba等人的大型LSTM语言建模为例。该网络具有两个层叠的LSTM层，其输入和状态的维度均为1500，并且具有10000字的词汇表的输出层。两个LSTM层的“ISS权重组”大小分别为24000和28000，验证集和测试集的困扰度分别为82.57和78.57。我们用$l_1$范数正则化来微调这个baseline LSTM。采用与baseline相同的训练超参数，除了更大的dropout保持比率为0.6（原始0.35）。因为$l_1$范数也是一种正则化，所以使用较弱的dropout率来避免过度拟合。过强的dropout加$l_1$范数正则化可能导致欠拟合。$l_1$范数正则化的权重衰减为0.0001。稀疏网络的验证困惑度和测试困惑度分别为82.40和78.60，与baseline大致相同。第一LSTM层，第二LSTM层和最后一个输出层的权重稀疏度分别为91.66％，90.32％和90.22％。图5绘出了学习的稀疏权重矩阵。顶行中的稀疏矩阵显示了一些有趣的模式：有很多全零列和行，并且它们的位置高度相关。这些模式在最下面一行进行分析。令我们惊讶的是，单独稀释个体权重可以收敛到稀疏的LSTMs，并消除了许多ISS - 第一和第二LSTM层中的504和220 ISS组件都是全零。

### APPENDIX B BIDAF中的ISS

![](https://i.loli.net/2018/04/17/5ad5a06bd4fc8.png)

### APPENDIX C BIDAF中的ISS权重组向量长度的直方图

图6：BiDAF中“ISS中的权重组”的矢量长度的直方图。 ISS学习到的BiDAF是表4第三行中的EM 66.32和F1 76.22。 使用我们的方法，长度被调整为更接近零点，零点处出现峰值，从而导致高ISS稀疏性。

![6](https://i.loli.net/2018/04/17/5ad5a086560fe.png)



## 注释

1. perplexity(困惑度)用来度量一个概率分布或概率模型预测样本的好坏程度。它也可以用来比较两个概率分布或概率模型。低困惑度的概率分布模型或概率模型能更好地预测样本。


2. ![ass](https://i.loli.net/2018/04/17/5ad5a0a1f27d6.png)

   红色的椭圆和蓝色的区域的切点就是目标函数的最优解，我们可以看到，如果是圆，则很容易切到圆周的任意一点，但是很难切到坐标轴上，因此没有稀疏；但是如果是菱形或者多边形，则很容易切到坐标轴上，因此很容易产生稀疏的结果。这也说明了为什么1范式会是稀疏的。

   很容易得出结论，ridge实际上就是做了一个放缩，而lasso实际是做了一个soft thresholding，把很多权重项置0了，所以就得到了稀疏的结果！

   除了做回归，Lasso的稀疏结果天然可以做机器学习中的另外一件事——特征选择feature selection，把非零的系数对应的维度选出即可，达到对问题的精简、去噪，以及减轻overfitting。 

3. 常规feedforward输入输出：矩阵

   输入矩阵形状：(n_samples, dim_input)

   输出矩阵形状：(n_samples, dim_output)

   真正测试/训练的时候，网络的输入和输出就是向量而已。加入n_samples这个维度是为了可以实现一次训练多个样本，求出平均梯度来更新权重，这个叫做Mini-batch gradient descent。 如果n_samples等于1，那么这种更新方式叫做Stochastic Gradient Descent (SGD)。

   ​

   常规recurrent输入输出：张量

   输入张量形状：(time_steps, n_samples,  dim_input)

   输出张量形状：(time_steps, n_samples,  dim_output)

   同样是保留了Mini-batch gradient descent的训练方式，但不同之处在于多了time step这个维度。

    recurrent的任意时刻输入的本质还是单个向量，只不过是将不同时刻的向量按顺序输入网络。所以你可能更愿意理解为一串向量 a sequence of vectors，或者是矩阵。

   ![ST](https://i.loli.net/2018/04/17/5ad5a0df1de0f.jpg)

   现在有一种更加有效的词向量模式，该模式是通过神经网或者深度学习对词进行训练，输出一个指定维度的向量，该向量便是输入词的表达。如[word2vec](https://code.google.com/p/word2vec/)。

4. Training RNN——BPTT

   ![PT](https://i.loli.net/2018/04/17/5ad5a0fba2874.png)

   ​

   5. 训练时，优化的核心思想是数据的读写（比如使矩阵贮存在寄存器中

      layer间合并/不同矩阵同类计算的合并