---
title: Residual Recurrent Neural Networks for Multivariate Time Series Forecasting

---

# R2N2：多变量时间序列预测的残差递归神经网络

## 摘要

多变量时间序列建模和预测是众多应用中的一个重要问题。诸如VAR（矢量自回归）模型等传统方法和诸如RNN等更近期的方法是建模时间序列数据中不可或缺的工具。在许多多变量时间序列建模问题中，通常有一个显着的线性相关分量，对于这些分量来说VAR是合适的，而非线性分量则适用于RNN。对仅有VAR或仅有RNN的时间序列进行建模可能导致较差的预测性能，或者训练时间较长的复杂模型。在这项工作中，我们提出了一个称为R2N2（残余RNN）的混合模型，它首先用简单的线性模型（如VAR）对时间序列进行建模，然后使用RNN对残差进行建模。

可以使用VAR和RNN的现有算法来训练R2N2。通过对两个数据集（航空和气候领域）的广泛实证评估，我们证明R2N2具有竞争力，通常比单独使用VAR或RNN更好。我们还表明，与RNN相比，R2N2训练速度更快，同时需要更少的隐藏单元数量。

## 1介绍

多变量时间序列建模和预测在医疗，金融，气候和航空等多个实际领域有许多应用，是一个重要的问题。因此，传统的时间序列模型包括自回归模型，如矢量自回归(VAR)和自回归整数移动平均(ARIMA)，基于潜在状态的模型如卡尔曼滤波器(KF)等广泛研究并用于多变量时间序列分析。虽然这些模型在多个领域有所帮助，但它们有一些关键的局限性：它们假定随时间变化的线性依赖关系，而且它们不适合模拟长期依赖关系。使用VAR等模型的另一个实际限制是可伸缩性和顺序选择问题，其中order表示历史依赖的假定持续时间。在拟合VAR模型时，必须运行适当的order选择网格搜索以选择数据的最佳可能order。对于一个跨越T时间步长的p维数据集，k阶VAR模型需要$O(T k^2p^4 + k^3p^6)$时间来估计其$O(kp^2)$参数。现在，如果模型在长期依赖关系下运行于高维度设置中，则网格搜索order选择的计算成本可能会很高。

为了拟合非线性时态数据，近年来基于递归神经网络（RNN）及其变体（门控循环单元GRU，长时间短期记忆LSTM）已经取得了成功。尽管这些工作大部分都是针对离散时序数据的，但这些模型已用于连续数据以及时序预测（参见第2节）。 RNN是具有建模长期依赖性的能力的非线性模型，而不需要明确指定确切的滞后/顺序。此外，RNN的参数数量不取决于数据展现的顺序，而主要取决于数据的维度。因此，这些模型不需要广泛的网格搜索程序来查找最佳顺序。但是，单独使用时间序列的RNN也存在问题。首先，他们很难训练得好，并可能遭受局部最小问题。即使仔细调整反向传播算法，也可能无法达到最佳参数。第二个问题是，如果数据具有显着的线性分量，与线性模型相比，RNN中增加的非线性复杂性可能实际上产生更差的结果。另一个问题是RNN的训练时间，这对于大型复杂模型可能很重要。

受这些问题的驱动，在这项工作中，我们提出了R2N2s（残余RNN）。 大多数现实世界的时间序列数据可以被假定为具有线性相关分量和非线性分量，其也可以捕获长期相关性。 为了处理这个问题，R2N2首先将一个简单的线性模型（如1阶VAR）拟合到数据中，然后使用线性模型中的误差残差作为RNN的输入数据。 因此，RNN只关注模拟VAR残差中剩余的非线性部分数据，以及任何长期相关性。 有两个阶段有效减轻了RNN的负担，同时提高了准确性并减少了train时间。

我们从两个领域获取真实世界的数据：航空工业（随着时间的推移，数百万飞行的多架飞机传感器测量）和气候科学（记录厄尔尼诺南方涛动几十年的指数），并对它们进行广泛的经验性评估。结果表明了R2N2s的多种优势：

（1）不必为VAR执行order选择，R2N2处理数据中的长期依赖性；

（2）R2N2的RNN组件需要较少数量的隐藏单元，以达到与单独使用RNN相同甚至更好的性能；

（3）R2N2的训练时间与单独训练RNN相比明显减少；

（4）与分别使用VAR或RNN相比，预测性能有所改善。

作为模型，R2N2的初始分量不一定是VAR，也可以使用其他基本模型，如卡尔曼滤波器（KF），自回归条件异方差（ARCH）模型和变体。因此，R2N2适用于不同类型的数据集，并且可以利用现有的，简单的，特定领域（线性）模型作为R2N2中的第一个组件。

本文的其余部分安排如下。第2节介绍了使用VAR和RNN进行时间序列预测的现有文献。在第3节中，我们讨论组件模型VAR，RNN和LSTM。接下来，在第4节中，我们将详细描述我们的模型，并在第5节中给出实验结果。最后，我们在第6节中总结该论文。

## 2相关工作

- 矢量自回归模型（VAR）：VAR模型及其扩展可以说是用于多变量时间序列预测的最广泛使用的统计模型系列之一。这些模型在各种应用中都有效，从描述经济和金融时间序列的行为到建模动力系统和估计大脑功能连通性等等。最近的工作已成功应用VAR模型来分析多变量航空时间序列数据，即航班的多个传感器测量。通过对多个机载传感器的时间演变进行建模，这些方法旨在精确预测飞机行为，从大型航班信息数据集中发现异常事件。
- 递归神经网络（RNN）：递归神经网络（RNN）在模拟时间序列数据方面非常有效。特别是LSTM网络，它是一种特殊类型的RNN，它们在时序数据上表现出了非凡的性能。由于它们能够学习顺序数据中的长期模式，因此它们最近已被应用于各种各样的问题，包括手写识别，机器翻译，文本分类，医疗诊断和其他。但是，大部分现有的工作都集中在离散的顺序数据和/或分类任务上。对于连续值时间序列建模，文献有一定的局限性，其中包括对情绪识别和在线音乐情绪回归的研究。除此之外，[22]的工作在Mackey Glass时间序列上使用了基于LSTM的预测模型，取得了可喜的成果。[7]中的一些较新的工作已经明确地集中在使用RNN进行概率时间序列预测的多元时间序列上。
- 混合模型：由于我们提出的模型（R2N2）是一种混合集成模型，即以适当的方式组合了两个或更多不同类型的基础模型的模型，我们在本节中回顾一些关于混合集合的现有工作。在[31]中，提出了一个使用自回归整数滑动平均（ARIMA）和前馈神经网络的残差模型。这个想法是，现实世界的时间序列数据是线性和非线性部分的混合体。 ARIMA对线性部分进行建模，而神经网络对其残差进行建模。在[20]中探讨了这种混合建模的另一种形式，在单变量时间序列数据上使用ARIMA和支持向量机（SVM）。然而，这两个作品都忽略了ARIMA残差的时间依赖性，并将它们作为独立的数据样本处理。相反，R2N2通过使用RNN考虑了残差的时间依赖性。另一项关于混合残差建模的工作[3]使用了Elman和NARX神经网络的组合。这里使用Elman网络对原始数据进行建模，然后使用更多Elman网络进行多级剩余建模，最后使用NARX网络来合并所有先前的输出。尽管残差中的时间依赖性不被忽略，但与R2N2不同，它们不考虑线性/短期和非线性/长期依赖性的组合。另一项最近的工作[12]提出了一种新的模型，它具有递归，卷积和自回归（AR）分量，所有这些分量都使用梯度下降法进行训练，以执行时间序列预测。虽然这项工作结合了AR和我们这样的RNN组件，但我们工作的不同之处在于AR组件并不是核心模型的一部分。相反，我们使用RNN来模拟AR部件的残差，这导致了不同的工作方向。实质，R2N2像梯度提升一样按顺序进行训练，尽管我们考虑混合集合，并且该模型用于多元时间序列预测。

![](https://i.loli.net/2018/04/21/5adae39b90a76.png)

> 图1：(a)图左侧说明模型的反馈回路，其中$x_t$，$h_t$和$y_t$是输入，隐藏状态和输出。右侧显示RNN时序展开，这也说明了训练反向传播步骤中梯度的流动; $E_t$是t时刻的误差。(b)颜色映射与(a)类似，表明这里的$h_t$等于RNN的隐藏状态$h_t$，$c_t$是添加到LSTM单元的新组件。这里的$h_t$可以用作单元格输出或使用前馈连接转换为输出$y_t$。σs表示细胞的门，它们是具有S形激活的单层神经网络。

## 3现有模型：VAR，RNN，LSTM

- 矢量自回归模型：矢量自回归（VAR）是一种流行的统计方法，用于模拟随时间演变的多个特征之间的线性相关性。在一般形式中，第k阶VAR模型可以写成:

  $x_t = A_1x_{t-1} + ... +A_kx_{t-k} + \epsilon_t$                                 (1)

  其中$A∈R^{p×p}$是系数矩阵，$x∈R^p$是参数向量，$\epsilon ∈ R^p$是零均值白噪声，$t = k + 1,...,T，$其中T表示时间序列的长度。下标k确定模型的滞后，即当前时间步中的数据依赖于过去的数据的程度。为了估计VAR参数，(1)中的模型通常被转换成适用于最小二乘估计的形式。

- 递归神经网络：给定时间序列数据$x_1,x_2,... ,x_t$，循环神经网络（RNN）由以下循环关系定义:

  $h_t = \sigma(Wx_t + Uh_{t-1} + b),$                                          (2)

  其中，$x_t∈R^p$是t时刻的输入，$W∈R^{n×p}，U∈R^{n×n}，b∈R^n$是隐式状态参数，$h_t$和$h_{t-1}∈R^n$是时刻t和t-1的隐状态向量 ，σ是logistic sigmoid函数。 图1a显示了一个RNN模型的例子，其主要部分是反馈回路，在时间步t从当前输入$x_t$和前一个隐藏状态$h_{t-1}$产生隐藏状态$h_t$。

  RNN模型的主要问题是，在训练过程中，当应用反向传播时间（BPTT）算法时，误差梯度会很快消失/爆炸。 这是由于在每个时间步骤应用分化的链式规则。 这会阻止网络捕获数据中的长期依赖关系。LSTM是RNN的扩展，作为一种机制通过反向传播恒定误差梯度来防止这些问题。

- 长期短期记忆：为了解决RNN的消失梯度问题，LSTM定义了一个新的隐藏状态，称为单元。每个单元都有自己的单元状态，这就像存储器一样，而各种控制机制称为门，可以修改单元存储器。 通过使用这样的机制，LSTM可以有效地学习何时忘记旧的状态（忘记门），何时从当前输入（输入门）添加新的状态以及提供什么作为当前单元的输出（输出门）。

  每个门是一个单层神经网络，其权重是训练期间要学习的额外参数。 门在其输出端具有S形激活，将输出约束到[0,1]范围。 这些值以分数表示读取，写入或忘记执行的次数，从而增加了模型的学习能力。

  基于LSTM的RNN由以下一组等式来管理：

  $i_t = σ (X_t · W_{xi} + h_{t−1} · W_{hi} + b_i)$

  $f_t = σ (X_t · W_{xf} + h_{t−1} · W_{hf} + b_f)$

  $o_t = σ (X_t · W{xo} + h_{t−1} · W_{ho} + b_o)$

  $u_t = tanh (X_t · W{xu} + h_{t−1} · W_{hu} + b_u)$

  $c_t = f_t\odot c_{t−1} + i_t\odot  u_t$

  $h_t = o_t\odot tanh(c_t)$

  其中，$f_t，i_t，u_t，c_t，o_t和h_t∈R^n$分别表示遗忘门，输入门，候选状态，单元状态，输出门和最终单元输出的输出。 $W_s，U_s和b_s$是训练期间学习的网络参数。 请注意，$h_t$本身可以用作网络的最终输出。 如果$h_t$不具有所需的尺寸，则可以使用完全连接的前馈神经网络将其转换为具有所需尺寸的输出$y_t$。 图1b显示了LSTM单元内的整个流程。

  LSTM不会遇到消失梯度问题的主要原因是因为单元状态$c_t$仅使用加法运算进行更新，而不涉及S形变换。 因此，在反向传播期间，只有一个不变的错误才会传播回每一步。 由于这些原因，LSTM能够学习数据中的远程依赖关系。

## 4残余回归神经网络

在本节中，我们描述我们提出的残差回归神经网络（R2N2）。一些现实世界的多变量时间序列数据通常包含一些可以通过线性模型捕获的方面，以及某些需要非线性可能的长存储器模型的其他方面。用于时间序列分析的经典模型，如VAR和variants对于具有短记忆的线性模型，即小量级VAR足够的线性模型都适用。然而，这样的模型不是为了捕获非线性和潜在的长时间内存依赖性而设计的。RNN适用于这种非线性和潜在的长时间内存依赖性。但，RNN通常需要较大的训练集和较长的训练时间。

R2N2背后的想法很简单。鉴于多个现实世界多变量时间序列模型确实同时具有线性和非线性依赖关系，R2N2使用线性模型来捕获线性依赖关系，线性模型的残差用于训练RNN。因此，R2N2既有线性分量又有非线性分量，其中线性部分负责模拟线性相关性，RNN侧重于线性模型的残余误差，这些非线性设计是非线性的。虽然可以直接在原始时间序列上训练RNN，但大部分建模工作可能致力于获得线性部分，这通常是几个实际时间序列数据的重要组成部分。R2N2使分量RNN只关注线性模型的残差，这有望导致更快的训练和更简单的模型，例如更少数量的隐藏单元。正如我们在第5节中所展示的那样，R2N2在原始时间序列上训练的完整RNN上确实显示了这些优势。

![](https://i.loli.net/2018/04/21/5adaead6cf67e.png)

>图2：(a)一般的R2N2模型的图解。块f表示基础模型，其是像VAR或卡尔曼滤波器那样的线性时间序列模型。 将这个模型的预测与基础事实进行比较以产生残差。 这些残差随后被输入RNN模型作为输入。然后RNN负责预测未来基础模型f产生的残差。这个预测残差可以加到f的预测中，以产生最终的校正输出。(b)以VAR作为线性模型和LSTM作为非线性模型的一种可能的R2N2实现。这是我们在实验中使用的模型。

图2a显示了某些特定地平线h的多元时间序列预测的R2N2模型的高层次说明。地平线是未来进行预测的时间量。这可以根据预测对于特定应用的有用性而变化。在该图中，f表示一个基本（线性）模型，如VAR或变体，它接收原始输入数据并进行第一级预测。这些预测与当时的基本事实进行比较以产生残差。多变量残差时间序列将包含（线性）基础模型无法有效建模的非线性和长期依赖性。非线性模型，这是一个递归神经网络（RNN），在这一点上接管。 RNN的任务是预测基础模型在预测下一个地平线的输出时将产生的误差。为此，RNN接收从基础模型到当前时间步骤的整个错误时间序列以及可能还有原始输入数据作为输入。一旦RNN预测到下一个时间段的误差，就可以将这与f的预测相结合，以生成模型的最终预测。请注意，对于线性模型充足的输入，残差很小，而RNN几乎没有;另一方面，如果线性模型产生较大误差，则RNN有效地接管建模。

根据应用域和计算能力的要求，可以很容易地实例化这个总体思想（参见图2b）。例如，如果时间序列是p维的并且被认为对时间的先前步骤具有线性依赖性，则基本线性模型可以是p个自回归（AR）模型或单个向量AR（VAR）模型的集合。另一方面，如果似乎存在控制数据的潜在状态，则可以将卡尔曼滤波器（KF）代入基本模型中。对于非线性RNN组件，可以使用诸如门控循环单元（GRU）或长期短期记忆（LSTM）模型之类的RNN变体，因为它们已证明具有良好的长期模式学习能力。此外，使用原始输入时间序列可以进一步增加基础模型预测的残差时间序列，从而使RNN能够做出更好的预测。这可能很有用，因为RNN知道基本模型所做的特定错误的真实数据。就方程而言，R2N2可以描述如下：

$\tilde x _{t+h} = f(x_{:t})$,                                (9)

$\tilde e_{t+h} = x_{t+h} - \tilde x_{t+h}$,                     (10)

$\hat e_{t+2h} = RNN([\tilde e_{:t+h},x_{:t+h}])$,     (11)

$\hat x_{t+2h} = f(x_{:t+h}) + \hat e_{t+2h}$,            (12)

我们使用:t或:t + h表示可以使用直至该时间步的数据。这里（9）表示由f表示的基础线性基础模型。线性模型将输入$x_{:t}$输入到时间步骤t，并生成输出$\tilde x_{t+h}$，这是模型对h的水平线的预测。在（10）式中，$x_{t+h}$表示时间t + h处的地面真值，利用它可以得到线性模型的残差$\tilde e_{t+h}$。 RNN被用作次要组件来模拟残差。方程（11）表明，RNN以到时间x+h为止与原始时间序列$x_{:t+h}$的残差$\tilde e_{t+h}$作为输入。 使用这些输入，RNN的任务是预测线性模型在下一次预测时间步t + 2h时将会产生多少误差。然后将该预测$\hat e_{t+2h}$与线性模型的预测$f(x_{:t+h})$相结合，以产生最终模型输出，如（12）所示。 上述等式中的相减和相加都是矢量量上的元素运算。

我们给出了图2b中R2N2模型的一个具体实现，VAR为基础模型，LSTM为非线性模型。这是我们在实验中用于这项工作的认识。对于现有的用于训练这种模型的算法，不需要改变，即VAR可以使用传统最小平方估计器来训练[16]，而LSTM使用梯度下降来训练[9]。这使我们能够利用现有的工作体系对这些模型进行培训，同时提供两者的新颖组合。

## 5实验

在本节中，我们将描述我们的数据集，实验设置并使用以下模型讨论结果：VAR，RNN（vanilla LSTM）和R2N2(带LSTM的残余RNN)。

### 5.1数据集

- 航空数据。第一个数据集是NASA的飞行运行质量保证（FOQA）数据集[2]。这些数据包含空中交通飞行传感器信息，并用于检测由于机械，环境或人为因素导致的飞机运行问题[18,24]。这些数据包含超过一百万次航班，每次航班都有约700次（多变量）时间序列测量的记录，在飞行期间以1Hz采样。这些参数包括来自控制开关（如推力，自动驾驶仪，飞行指引器等）和传感器（如高度，迎角，漂移角度等）的离散和连续读数。对于我们的实验，我们选择了42个特征，代表来自相同类型飞机的160个航班的所有连续传感器测量结果，这些特征落在同一机场。重点是在10,000英尺以下直到触地（持续时间600-1500个时间戳）的飞行的一部分，这总共产生大约120,000个时间步长的数据。我们将其分为训练，验证和5个测试组，分别在每个测试组中有100,10和10个飞行。
- ENSO数据。我们的第二个数据集代表厄尔尼诺南方涛动（ENSO）数据。 ENSO循环是指在赤道太平洋发生的海面温度，降水，地面气压和大气环流的逐年变化，有时甚至是非常强烈[1]。该数据集由NINO 1 + 2，NINO 3.4等7个指数组成，每月的测量数据从1950年到2008年不等。每个特征代表不同地区的海表温度测量（或其组合）。总而言之，这包括707个多变量测量，其分为训练集（60％），验证集（20％）和测试集（20％）。

### 5.2实验细节

航空数据通过z评分进行标准化，使其具有零均值和单位方差。我们对所有变量进行一步提前预测，这意味着预测范围h = 1。所有超参数选择都是使用一个保留的验证集进行的，报告的结果是5个保留测试集的平均性能。

在ENSO数据集的情况下，我们提前6个月进行预测，这意味着VAR的最小可能滞后期为6。我们将其表示为该数据集的VAR-1。由于这是一个气候数据集，因此在某些特征中会出现年度周期性结构。为了说明这一点，在训练模型之前，我们从整个时间序列中减去训练数据的月平均值，这有助于产生更好的预测。在将预测数据转换回原始比例后报告结果。此外，由于这是一个非常小的数据集，我们需要用递归权重的L2范数来严格规范RNN，以防止过度拟合。超验参数选择再次在验证集上进行，结果报告给测试集。

对于这两个数据集，通过增加滞后量直到误差停止下降，同时从{0.05,0.5,5.0,50.0,500.0}网格搜索最佳正则化参数，找到最佳的VAR。 R2N2总是使用VAR-1作为基础模型。对于RNN和R2N2，我们使用基于LSTM的单元格并通过网格搜索确定隐藏层的最佳大小。至于RNN的深度，我们发现单层表现最好，并且是最快的训练，而不是使网络更深入，这不会带来更多收益。 Adam [11]算法被用来训练RNN。每当验证损失停止改进时，学习率降低10倍。这导致更好的表现，同时也防止过度配合。当学习率低于某一阈值时停止训练，并且在验证损失中未观察到进一步的改进。

我们使用了两个评估指标：

平均相对平方误差（MRSE）定义为:

$MRSE = \frac{\sqrt{\sum_{i=1}^n \sum_{t=1}^T(X_{it}-\hat X_{it})^2}}{\sqrt{\sum_{i=1}^n \sum_{t=1}^T(X_{it} - mean(X_i))^2}}$

相对误差（RE）定义为:

$RE = \frac{\sqrt{\sum_{i=1}^n \sum_{t=1}^T(X_{it}-\hat X _{it})^2}}{\sqrt{\sum_{i=1}^n \sum_{t=1}^T(X_{it})^2}}$

其中$X，\hat X∈R^{n×T}$分别为地面实况和模型预测。$X_{it}$表示时间t处的第i维的值。两个指标的值越低越好。与预测特征的均值相比，MRSE衡量模型预测的好处。RE测量相对于原始数据的比例的误差。

### 5.3结果

#### 5.3.1定性预测

图3显示了从航空数据集随机选择的航班中选择的特征的预测图（所有42个特征的等效图均包含在补充材料中）。该图显示了VAR-1，RNN和R2N2的基础事实和预测（以VAR-1为基础模型）。R2N2对于高度易失性和“平滑”特征都表现良好，而RNN与平滑特征以及具有易失性特征的VAR-1一起努力。这也被其他功能注意到了。因此，在某种意义上，VAR-1可以被认为是为R2N2提供了稳定性，同时仍然赋予它足够的灵活性来模拟高波动性。

图4显示了来自ENSO数据集的选定指数的实际预测和预测残差图（所有7个指数的等效图均包含在补充材料中）。在ENSO数据集的7个指数中，RNN和R2N2均显着优于VAR，这是预期的，因为这些指数是高度非线性现象。对于7个中的2个索引，RNN优于R2N2。这些指数（NINO 1 + 2和NINO 3）由于受到季节的影响而呈现高度周期性的结构。在减去月平均值之后，RNN能够很好地专注于对剩余部分进行建模（这本身就是一个非常简单的残差模型）。另一方面，对于没有这种周期性结构的4个指数来说，平均相减没有多大帮助，而R2N2仍然表现良好。图4还显示了底行的预测残差图。请注意，对于NINO 1 + 2，尽管RNN表现最好，但R2N2在其预测中仍偏离VAR，并且更接近于RNN。这表明R2N2在某种意义上，擅长捕捉“两全其美”。总体而言，在这三种方法中，R2N2在数据集和所有特征中都是最好的或次佳的。

![](https://i.loli.net/2018/04/21/5adaf34262a06.png)

> 图3：来自航空数据集的选定特征的样本预测图。 曲线图例中圆括号中的值是该特征的MRSE（对于所有其他曲线也是如此）。 请注意，对于左图，这是一个具有高波动性的特征，VAR表现不佳，但R2N2持续改善其误差。 另外，R2N2和RNN在性能上相当。 对于相对“平滑”或不易挥发特征的右图，VAR表现相当好。 R2N2仍然试图改善一点，而RNN的预测非常不稳定。

#### 5.3.2综合定量表现

图5a显示了我们在航空测试集上获得的MRSE的平均值。R2N2-128（即以VAR-1为基础模型的R2N2和隐藏层大小为128的RNN）表现最好。VAR-5与RNN-128完全相同，这表明正则化的正确顺序选择对于VAR来说是非常必要的，以实现良好的性能。另外请注意，与其他模型相比，VAR-1的MRSE较高，但我们仍然建议将其用作R2N2的基本模型。如果使用更高阶VAR（如VAR-5）作为R2N2的一部分，则无法改进VAR-5的性能。一个可能的解释可能是，当用作基准模型时，拟合良好的VAR将R2N2偏向“VAR操作域”，而R2N2无法做出进一步的改进。使用更简单的VAR-1为R2N2提供了良好的基础模型，同时为RNN部分提供了足够的灵活性。这也使得R2N2模型不必为VAR组件执行订单选择。图5b显示了通过每月添加指数的方法将预测数据转换回原始比例后ENSO数据集的相同结果。 RE的结果与MRSE类似，所以为了空间的利益，它们被显示在补充材料中。

![](https://i.loli.net/2018/04/21/5adaf3f2d579d.png)

> 图4：ENSO数据集中选定特征的样本预测图。 第一行包含实际的数据和模型预测。R2N2在第一个指数（NINO 3.4）中表现最好，而在第二个（NINO 1 + 2）的情况下RNN表现最好，这是高度周期性的。 最下面一行显示了每个模型的预测与地面实况相比较的残差图（无误差为0）。 请注意，R2N2减少了VAR-1造成的错误。 此外，对于NINO 1 + 2，该图显示RNN获胜，但R2N2接近RNN并远离VAR。

![](https://i.loli.net/2018/04/21/5adaf437d3ffc.png)

> 图5：（a）所有3个模型 - VAR，RNN和R2N2的航空测试集的平均MRSE结果。 请注意，对于VAR，我们显示了1阶结果和最佳阶次结果（在这种情况下为5）。 RNN-128表示隐藏层大小为128的RNN。R2N2-128使用VAR-1作为基础模型，隐含层大小为128的RNN。R2N2-128在所有模型体系结构中表现最佳。 两个图中的误差线代表由于跨多次运行的参数的不同初始化导致的变化性。 棒条很小，表明RNN和R2N2的性能都很好。 （b）MRSE对所有模型的ENSO数据结果。 R2N2-64在所有型号架构中表现最佳。 （c）针对航空数据的RNN和R2N2不同隐藏层大小的MRSE值。 请注意，即使R2N2-32的性能也优于RNN-128。 这意味着可以将非常低复杂度的RNN用作R2N2中的组件，这在计算复杂性方面提供节省。

#### 5.3.3分析训练时间：RNN vs R2N2

接下来我们将研究在航空数据集上跨不同体系结构训练RNN和R2N2所需的时间。通过在本节中训练R2N2的时间，我们参考R2N2的RNN分量的训练时间，而RNN的训练时间是指仅RNN模型的训练时间。与RNN相比，R2N2的VAR部分的训练时间可以忽略不计（数量级减少），所以我们在本次讨论中单独提及它。图6显示了RNN和R2N2不同大小隐藏层的学习曲线。该图清楚地表明，与普通RNN相比，R2N2在实现最小测试集错误方面速度非常快。对于R2N2s，我们可以在开始时观察到急剧下降，这可以归因于基础VAR模型。在第一次迭代之后，R2N2发现它已经完成了很多工作。相反，对于RNN，我们可以看到，在最终测试集损耗高于R2N2时，损耗逐渐减少，然后逐渐减少。该实验验证了我们之前提出的将VAR用作R2N2中的基本模型减轻了RNN组件的负担。 RNN组件快速模拟VAR的残差并更快地收敛相对要容易得多。

![](https://i.loli.net/2018/04/21/5adaf4bb73cf3.png)

> 图6：这些曲线图描述了在每个模型的4次不同初始化过程中训练迭代进行时航空测试集上损失的发展。 请注意，R2N2的训练速度比RNN快得多。 另外，由于VAR的协助，开始时R2N2的损失急剧下降。

#### 5.3.4架构比较：RNN vs R2N2

在这个实验中，我们使用航空数据集来研究修改RNN结构对预测性能的影响。同样，当我们谈论RNN时，它指的是仅RNN模型，而R2N2指的是R2N2的RNN组件。图5c显示了修改MRSE上隐藏层中节点数量的效果。随着RNN和R2N2隐藏层大小的增加，性能一直在提高。然而，更有意思的是，即使R2N2中的32节点隐藏层的性能也比RNN中的128节点隐藏层要好。这个结果表明，由于使用VAR作为基本线性模型，RNN的输入变得不那么复杂，并且可以用更简单的体系结构来建模。结合前面关于时间剖面的结果，可以在训练时间内减少两倍（复杂度较低的模型和更快的收敛），同时提供更好的性能。 RE的结果与MRSE类似，由于缺乏空间，它们在辅助材料中。

## 6结论

在这项工作中，我们将传统的时间序列模型与递归神经网络相结合，提出了一个新的模型R2N2，用于多变量时间序列预测。我们的想法是使用现有的经典时间序列分析机制对时间序列进行建模，然后使用RNN对残差进行建模。我们用VAR作为基本线性模型和基于LSTM的RNN作为非线性部分探讨了该模型的具体实现。通过对来自航空和气候领域的两个真实世界数据集进行广泛的实验，我们证明R2N2s单独获得比VAR和RNN更好的预测性能。此外，我们还通过实验证明R2N2需要更简单的神经网络架构，从而显着减少训练时间。 R2N2的另一个优势是可以通过使用特定于领域的基本线性模型（如卡尔曼滤波器，ARCH模型等）轻松地定制到不同的域，为未来工作铺平了道路。