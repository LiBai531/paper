---
title: Capsule Network

---

# Capsule Network

## **Part I: Intuition.**

### 1.介绍

上周，Geoffrey Hinton和他的团队发表了两篇论文，介绍了一种基于所谓胶囊的全新型神经网络。除此之外，团队还发布了一个算法，称为胶囊之间的动态路由，这个算法用来训练这样一个胶囊网络。

对于深度学习社区的每一个人来说，这都是一个巨大的消息，有几个原因。首先，Hinton是深度学习的创始人之一，也是当今广泛使用的众多模型和算法的发明者。其次，这些论文引入了一些全新的东西，这是非常令人兴奋的，因为它很可能激发更多的研究和非常酷的应用。

在这篇文章中，我将解释为什么这个新架构如此重要，以及它背后的直觉。

然而，在谈论胶囊之前，我们需要看看CNN，这是当今深度学习的主力。

### 2.CNN有重要的缺点

CNN（卷积神经网络）真棒。这是今天深度学习如此受欢迎的原因之一。他们可以做出令人惊叹的事情，人们曾经认为计算机长期以来都无法做到这一点。尽管如此，它们有其局限性，它们有根本的缺陷。

让我们考虑一个非常简单和非技术性的例子。想象一张脸，有什么组成部分？我们有椭圆形的脸，两只眼睛，一个鼻子和一个嘴巴。 对于一个CNN来说，这些对象的存在可以成为一个非常有力的指标，可以认为图像中有一张脸。这些组件之间的定向关系和相对空间关系对CNN来说并不重要。

CNN如何工作？ CNN的主要组成部分是一个卷积层。它的工作是检测图像像素中的重要特征。较深层（接近输入）的图层将学习检测诸如边缘和颜色渐变等简单特征，而较高层将简单特征组合成更复杂的特征。最后，网络顶部的密集层将结合非常高级的特征并产生分类预测。

需要了解的重要一点是，较高级别的特征作为较低级特征的加权组合：前一层的激活与下一层神经元的权重相乘并相加，然后传递到激活非线性。在这个设置中，没有任何地方在构成更高级特征时考虑了简单特征之间存在姿态（平移和旋转）关系。CNN解决这个问题的方法是使用最大汇集或连续卷积层来减少流经网络的数据的空间大小，从而增加高层神经元的“视野”，从而允许他们在较大的感受域上检测高阶特征。Max Pooling是一个使卷积网络工作得非常好的依仗，在许多领域实现了超人的表现。但是不要被它的表现所迷惑：虽然CNN比之前的任何模式都要好，但是Max Pooling正在丢失有价值的信息。

Hinton自己说，Max pooling运作良好的事实是一个很大的错误和灾难：

> Hinton：“卷积神经网络中使用的汇集操作是一个很大的错误，它运行得很好的事实是一场灾难。”

当然，你可以在传统CNN上取消最大池化，仍然能得到好的结果，但是这仍然不能解决关键问题：卷积神经网络的内部数据表示不考虑简单和复杂对象之间的重要空间层次。

上面的例子中，图片中仅存在2只眼睛，嘴巴和鼻子并不意味着有脸，我们也需要知道这些物体之间的相对关系。

### 3.将3D世界硬编码为神经网络：逆向图形方法

计算机图形学涉及从几何数据的内部分层表示来构造可视图像。请注意，这种表示的结构需要考虑对象的相对位置。这些内部表示作为表示这些对象的相对位置和方向的几何对象和矩阵的阵列存储在计算机的存储器中。然后，特殊软件将这些表示转换成屏幕上的图像。这就是所谓的渲染。

受这个想法的启发，Hinton认为，大脑事实上与渲染相反。他称之为逆向图形：从眼睛接收到的视觉信息中，他们解构了我们周围世界的等级表征，并试图将其与已经学习的模式和存储在大脑中的关系相匹配，这就是如何识别的过程。关键的思想是大脑中物体的表示不依赖于视角。

所以现在的问题是：我们如何建模神经网络中的这些层次关系？答案来自计算机图形学。在3D图形中，3D对象之间的关系可以用所谓的姿势表示，其本质上是平移加旋转。

Hinton认为，为了正确地进行分类和对象识别，重要的是保持对象各部分之间的分层姿态关系。这是让你理解胶囊理论为何如此重要的关键直觉。它结合了对象之间的相对关系，并以数字形式表示为4维姿态矩阵。

当这些关系被构建到数据的内部表示中时，模型变得非常容易理解，它所看到的只是另一种以前看到的东西。考虑下面的图片，你可以很容易地认识到，这是自由女神像，即使所有的图像从不同的角度显示。这是因为你脑中的自由女神像的内部表现并不取决于视角。你可能从来没有见过这些照片，但你仍然立即知道它是什么。

对于一个CNN来说，这个任务确实很难，因为它没有这个3D空间的内建理解，但是对于一个CapsNet来说，这是非常容易的，因为这些关系是明确建模的。使用这种方法的论文相比于先前的技术水平能够将错误率降低45％，这是一个巨大的改进。

胶囊方法的另一个好处是它能够通过使用CNN所使用的数据的一小部分来学习获得最先进的性能（Hinton在他著名的关于CNN错误的著名讲话中提到了这一点）。从这个意义上说，胶囊理论更接近人脑在实践中所做的事情。为了学会把数字分开，人脑只需要看几十个例子，最多只有几百个例子。另一方面，CNN需要成千上万的例子才能取得非常好的成绩，这看起来像是一种暴力方式，显然比我们的大脑差。

### 4.什么使CNN持续这么久？

这个想法非常简单，为什么没有人以任何方式提出过它！而事实是，Hinton几十年来一直在思考这个问题。之所以没有出版物，是因为以前没有技术上的办法。其中一个原因是，在2012年左右之前，在GPU之前的时代，计算机还不够强大。另一个原因是，没有一种算法可以实现并成功学习胶囊网络（同样，人造神经元自1940年代就开始出现，但是直到1980年代中期，反向传播算法才出现并允许成功训练深度网络）。

同样，胶囊本身的想法并不是那么新，而Hinton之前也提到过，但是到目前为止，还没有一种算法能够实现。而我们的算法被称为“胶囊之间的动态路由”。该算法允许胶囊相互通信，并在计算机图形中创建与场景图相似表示。

### 5.结论

胶囊引入了一个新的构建模块，可以在深度学习中使用，以更好地模拟神经网络内部知识表示内部的层次关系。他们身后的直觉非常简单而优雅。

Hinton和他的团队提出了一种训练这种由胶囊组成的网络的方法，并成功地在一个简单的数据集上进行了训练，实现了最先进的性能，这是非常令人鼓舞的。

尽管如此，挑战依旧存在。胶囊目前的实现比其他现代深度学习模型慢得多。时间会证明胶囊网络是否可以被快速有效地训练。另外，我们需要看看它们是否适用于更困难的数据集和不同的领域。

无论如何，胶囊网络是一个非常有趣而且已经有效的模型，随着时间的推移，它肯定会得到更多的发展，并有助于深度学习应用领域的进一步扩展。

## PartII: How Capsules Work.

### 1.什么是胶囊？

为了回答这个问题，我认为引用胶囊被介绍的第一篇论文--Hinton等人的“Transening Autoencoders”是一个好主意。下面提供了对于理解胶囊很重要的部分：

> “神经元的活动中使用单个标量输出来总结局部复制特征检测器的活动，而不是针对观察者的不变性。事实上神经网络应该使用局部“胶囊”来执行一些相当复杂的内部计算他们的输入，然后将这些计算的结果封装成一个高度信息量输出的小向量。每个胶囊学习识别在观察条件和变形的有限范围上的隐式消隐的视觉实体，并且输出实体存在于其有限的范围内的概率和一组“实例化参数”，其可以包括精确的姿态，照明以及视觉实体相对于该实体的隐式删除规范版本的变形。当胶囊正常工作时，视觉实体存在的概率是局部不变的——当胶囊覆盖的有限范围内的实体在可能的外观上移动时，它不会改变。然而，实例化参数是“等变的” ——当观察条件改变并且实体在外观流形上移动时，实例化参数改变相应的量，因为它们表示实体在外观流形上的固有坐标。” 

上面的段落信息量很大，我花了一段时间来逐句说清楚它的意思。以下是解读：

人造神经元输出一个标量。另外，CNN使用卷积层，对于每个内核，在整个输入体积内复制相同内核的权重，然后输出一个2D矩阵，其中每个数字是内核与输入体积的一部分卷积的输出。所以我们可以把这个二维矩阵作为复制特征检测器的输出。 然后，所有内核的2D矩阵被堆叠起来以产生卷积层的输出。

然后，我们试图在神经元的活动中实现视点不变性。我们通过连续查看上述二维矩阵中的区域并选择每个区域中的最大数目的最大池的方法来做到这一点。结果，我们得到了我们想要的活动的不变性。不变意味着通过改变输入一点，输出仍然保持不变。活动只是神经元的输出信号。换句话说，在输入图像中，我们将要检测的对象稍微移动一下，网络活动（神经元的输出）因为max Pooling的存在不会改变，网络仍然会检测到对象。

上述机制不是很好，因为Max Pooling会丢失有价值的信息，也未编码特征之间的相对空间关系。我们应该使用胶囊，因为它们将以矢量形式（而不是神经元输出的标量）封装关于它们正在检测的特征状态的所有重要信息。

胶囊封装了所有他们以向量形式检测到的特征状态的重要信息。

胶囊将特征检测的概率作为其输出向量的长度进行编码。检测到的特征的状态被编码为该向量指向的方向（“实例化参数”）。因此，当检测到的特征在图像周围移动或者其状态以某种方式改变时，概率仍然保持不变（矢量长度不变），但是其方向改变。 

想象一下，胶囊检测到图像中的脸部并输出长度为0.99的3D矢量。然后我们开始在整个图像上移动脸部。矢量将在其空间中旋转，表示检测到的脸部的变化状态，但其长度将保持固定，因为胶囊仍然确定已经检测到脸部。这就是Hinton所说的活动等同性：当一个对象发生变化时，神经元活动就会发生变化，“在可能出现的多种外观上移动”。与此同时，检测的概率保持不变，这是我们应该针对的不变性形式，而不是CNNs最大池化的形式。

### 2.胶囊是如何工作的？

让我们比较胶囊与神经元。下表总结了胶囊与神经元之间的区别：

![](https://i.loli.net/2018/04/25/5ae03d84f22c0.png)                              

回想一下，神经元从其他神经元接收输入标量，然后乘以标量权重和总和。然后将这个总和传递给许多可能的非线性激活函数中的一个，这些非线性激活函数采用输入标量并根据函数输出标量。该标量将作为输出输入到其他神经元。这个过程的总结可以在右边的表格和图表中看到。实质上，人造神经元可以用三个步骤来描述：

1、输入标量的标量加权；

2、加权输入标量之和；

3、标量到标量的非线性；

 另一方面，除了新的步骤之外，胶囊具有上述3个步骤的矢量形式，输入的仿射变换：

1，输入向量的矩阵乘法；

2，输入向量的标量加权；

3，加权输入向量之和；

4，矢量到矢量的非线性；

让我们更好地了解胶囊内部发生的4个计算步骤。

 ![](https://i.loli.net/2018/04/25/5ae03de00f548.png)

1.输入向量的矩阵乘法

我们的胶囊收到的输入矢量（图中的u1，u2和u3）来自下面三层中的其他胶囊。这些矢量的长度对下层胶囊检测其相应对象的概率进行编码，并且矢量的方向对检测到的对象的内部状态进行编码。 让我们假设较低级别的胶囊分别检测眼睛，嘴巴和鼻子，然后输出胶囊检测脸部。

然后这些矢量乘以编码较低级特征（眼睛，嘴和鼻子）和较高级特征（脸）之间的重要空间和其他关系的相应权重矩阵W. 例如，矩阵W2j可以编码鼻子和脸部之间的关系：脸部以鼻子为中心，其大小是鼻子大小的10倍。鼻子及其在空间中的取向对应于鼻子的取向，因为它们全都位于同一平面上。可以为矩阵W1j和W3j绘制类似的直觉。乘以这些矩阵后，我们得到的是更高级特征的预测位置。换句话说，u1根据检测到的眼睛位置来表示面部的位置，u2表示根据检测到的嘴部位置来确定面部的位置，u3表示根据检测到的鼻子位置来确定的面部特征。

 ![](https://i.loli.net/2018/04/25/5ae03e1b15d04.png)

在这一点上，直觉应该是这样的：如果这三个低层特征的预测指向同一个位置和脸的状态，那么它必定是一个脸。

2.输入向量的标量加权

乍一看，这一步似乎对于神经元在加入之前对其输入进行加权的方法非常熟悉。在神经元的情况下，这些权重是在反向传播期间学习的，但是在胶囊的情况下，它们是使用“动态路由”确定的，这是一种确定每个胶囊的输出在哪里进行的新方法。我将专门给这个算法分配一个帖子，在这里只提供一些直觉。

 ![](https://i.loli.net/2018/04/25/5ae03e4c0136a.png)

在上面的图片中，我们有一个较低级别的胶囊需要“决定”哪个更高级别的胶囊将发送它的输出。它将通过调整权重C做出决定，该权重将在将胶囊的输出发送到左侧或右侧的更高级别胶囊J和K之前将该胶囊的输出相乘。现在，更高级别的胶囊已经接收到来自其他更低级别胶囊的许多输入向量。所有这些输入都以红点和蓝点表示。这些点聚集在一起，这意味着较低级别胶囊的预测彼此接近。这就是为什么，例如，在胶囊J和K中都有一组红点。

那么，我们的低级胶囊应该把它输出到胶囊J还是胶囊K？ 这个问题的答案是动态路由算法的本质。当下层胶囊的输出乘以相应的矩阵W时，远离胶囊J中的“正确”预测的红色簇。另一方面，如果将落在与“正确”预测非常接近的K胶囊中的红色簇。下层胶囊具有测量哪个上层胶囊更好地适应其结果的机制，并且将以这样的方式自动调整其权重，即与胶囊K相对应的权重C将是高的，并且与胶囊J相对应的权重C将是低的。

### 3.加权输入向量的和

这一步与常规的人工神经元相似，代表了输入的组合。我不认为这一步有什么特别之处（除了是向量和标量的总和）。因此，我们可以继续下一步。

### 4.“壁球”：新的矢量对矢量非线性

CapsNet引入的另一个创新是采用矢量的新型非线性激活函数，然后“压扁”它的长度不超过1，但不改变方向。

 $V_j = \frac{\left \| s_j \right \|^2}{1 + \left \| s_j \right \|^2} \frac{s_j}{\left \| s_j \right \|^2}$

公式的右侧缩放输入矢量，它将具有单位长度，左侧执行附加缩放。请记住，输出矢量长度可以解释为胶囊检测到给定特征的概率。

左边是应用于一维向量的压缩函数，这是一个标量。我用它来演示函数的有趣的非线性形状。只有将一维情况可视化才有意义，实际将采取矢量和输出一个向量，这将很难形象化。

![](https://i.loli.net/2018/04/25/5ae03f3357ca3.png)

### 5.结论

在这一部分，我们讨论了胶囊是什么，它执行的是什么样的计算，以及它背后的直觉。我们看到胶囊的设计建立在人工神经元的设计之上，但将其扩展到矢量形式以允许更强大的代表性能力。它还引入了矩阵权重来编码不同层次的特征之间的重要层次关系。结果成功地实现了设计者的目标：关于输入变化和特征检测概率的不变性的神经元活动等变性。

![](https://i.loli.net/2018/04/25/5ae03ffcb16b9.png)

![](https://i.loli.net/2018/04/25/5ae043063a5b8.png)

## PartIII: Dynamic Routing Between Capsules.

### 1.介绍

这是关于基于胶囊的新型神经网络系列的第三篇文章，名为CapsNet。在这篇文章中，我将讨论允许训练胶囊网络的新型动态路由算法。

正如我在第二部分中所展示的那样，下层的胶囊需要决定如何将其输出向量发送到更高级别的胶囊。它通过改变标量权重$c_{ij}​$来做出这个决定，这个标量权重将会乘以它的输出向量，然后被当作是输入到一个更高级别的胶囊中。$c_{ij}​$表示将来自较低级别胶囊$i​$的输出矢量相乘并作为输入到较高级别胶囊$j​$的权重。

关于权重$c_{ij}$：

1、每个重量是一个非负的标量;

2、对于每个较低级别的胶囊$i$，所有权重$c_{ij}$的总和等于1;

3、对于每个较低级别的胶囊$i$，权重的数量等于较高级别胶囊的数量;

4、这些权重由迭代动态路由算法确定。 

前两个事实允许我们用概率术语解释权重。回想一下胶囊的输出向量的长度被解释为这个胶囊已经被训练检测的特征的存在概率。输出矢量的方向是特征的参数化状态。所以，从某种意义上说，对于每个较低级别的胶囊，其权重$c_{ij}$定义了属于每个较高级别的胶囊$j$的输出的概率分布。

### 2.胶囊之间的动态路由

那么，在动态路由中究竟发生了什么呢？ 我们来看看本文发表的算法的描述。但是在我们逐步深入研究算法之前，我希望您能够牢记算法背后的主要直觉：

**较低级别胶囊会将其输入发送到与其输入“一致”的较高级别胶囊。 这是动态路由算法的本质。**

![img](https://i.loli.net/2018/04/25/5ae0413b4b7d5.jpg)

第一行表示这个过程将所有的胶囊都放在较低的级别l和他们的输出$\hat{u}$，以及路由迭代次数$r$。 最后一行告诉你，算法将产生一个更高级别胶囊$v_j$的输出。 本质上，这个算法告诉我们如何计算网络的正向传递。

在第二行，你会注意到有一个新的系数$b_{ij}$，我们以前没有见过。 这个系数只是一个临时值，将被迭代更新，并且在过程结束后，它的值将被存储在$c_{ij}$中。训练开始时，$b_{ij}$的值初始化为零。

第3行表示4-7中的步骤将被重复r次（路由迭代次数）。

第4行中的步骤计算矢量$c_i$的值，它是所有较低级别胶囊$i$的路由权重。这是为所有较低级别的胶囊完成的。为什么选择softmax？ Softmax将确保每个权重$c_{ij}$是一个非负数，它们的和等于1。本质上，softmax强化了我上面描述的系数$c_{ij}$的概率性质。

在第一次迭代中，所有系数$c_{ij}$的值将是相等的，因为在第二行上所有的$b_{ij}$被设置为零。例如，如果我们有3个低级别胶囊和2个高级别胶囊，则所有$c_{ij}$将等于0.5。在算法初始化时，所有$c_{ij}$的状态都是相等的，表示最大的困惑和不确定性状态：较低级别的胶囊不知道哪个较高级别的胶囊最适合它们的输出。当然，随着这个过程的重复，这些均匀分布将会改变。

在对所有较低级别的胶囊计算了所有的权重$c_{ij}$之后，我们可以移动到第5行，在那里我们查看更高级别的胶囊。这个步骤计算输入向量的线性组合，由前一步确定的路由系数$c_{ij}$加权。直观地说，这意味着缩小输入矢量并将它们加在一起，这产生输出矢量$s_j$。这是为所有更高级别的胶囊完成的。

接下来，在第6行中，来自最后一步的矢量经过了南北非线性，这确保了矢量的方向被保留，但是其长度被强制为不大于1。该步骤产生所有更高级别胶囊的输出矢量$v_j$。

总结到目前为止：步骤4-6只是计算更高级别胶囊的输出。第7行是更新权重的地方。这一步捕获了路由算法的本质。此步骤查看每个更高级别的胶囊j，然后检查每个输入并根据公式更新相应的权重$b_{ij}$。公式说明新权重值等于旧值加上胶囊j的当前输出的点积和从较低级别胶囊i输入到该胶囊的输入。点积看着胶囊的输入和胶囊的输出之间的相似性。另外，从上面记得，低级别胶囊会将其输出发送到输出类似的高级别胶囊。这种相似性被点积所捕获。在此步骤之后，算法从步骤3开始重复该过程r次。

在r次之后，计算更高级别胶囊的所有输出并且建立路由权重。正向传递可以延续到下一级的传递。

权重更新步骤的直观示例

![img](https://i.loli.net/2018/04/25/5ae0413b89af6.jpg)

在图中，假设有两个更高级别的胶囊，它们的输出由紫色矢量v1和v2表示，如前一节所述。橙色矢量表示来自其中一个较低级别胶囊的输入，而黑色矢量表示来自其他较低级别胶囊的所有其余输入。

我们看到在左边部分的紫色输出$v_1$和橙色输入$\hat{u}$指向相反的方向。换句话说，它们并不相似。这意味着它们的点积将是一个负数，结果路由系数$c_{11}$会减少。在右边部分，紫色输出$v_2$和橙色输入$\hat v$指向相同的方向。他们是相似的。因此，路由系数$c_{12}$会增加。对于所有更高级别的胶囊和每个胶囊的所有输入重复该过程。这个结果是一组路由系数，它们与来自较低级别胶囊的输出与较高级别胶囊的输出最匹配。

### 3.路由迭代次数？

该论文验证了MNIST和CIFAR数据集的一系列值。作者的结论是双重的：

1、更多的迭代往往会使数据过度拟合;

2、建议在实践中使用3次路由迭代。

### 4.结论

在这篇文章中，我通过协议解释了动态路由算法，该协议允许训练CapsNet。最重要的思想是，输入和输出之间的相似性被测量为胶囊的输入和输出之间的点积，然后相应地更新路由系数。最佳做法是使用3次路由迭代。